# AI Video Editor

This project aims to take a single long-form source video and automatically craft an edited cut using ffmpeg driven by large language models.


## Ingredients

### MLX Models (Apple Silicon - Recommended)
- **Video analysis: `mlx-community/Qwen3-VL-30B-A3B-Thinking-4bit`** (4-bit quantized, ~18GB)
- **Planning: `Qwen/Qwen3-14B-MLX-4bit`** (4-bit quantized)
- Framework: [MLX](https://github.com/ml-explore/mlx)
- **Requirements**: Mac with Apple Silicon (M1/M2/M3), 64GB RAM recommended

- ### MLX Models (Apple optimized)
- **Video analysis: `mlx-community/Qwen3-VL-30B-A3B-Thinking-4bit`
- **Planning: `Qwen/Qwen3-14B-MLX-4bit`
- **Note**: These are Qwen3 models ONLY. Do not use Qwen2 models.

## Features

The CLI currently automates the entire post-production loop:

1. **Chunking** – Splits a long source video into evenly sized segments via `ffmpeg`.
2. **Chunk analysis** – Invokes Video Analysis agent to describe every segment, highlight dull moments, and capture people metadata into markdown.
3. **Director plan** – Summarises all chunk reports with Planning agent to produce a structured editing script (JSON + Markdown).
4. **ffmpeg execution** – Applies trims, optional crops, speed ramps, overlays, subtitles, and background music cues (mixed from the local royalty-free library) to produce polished chunk edits.
5. **Final assembly** – Concatenates the edited chunks into one final deliverable.

## Quick start

### For Apple Silicon (Recommended)

Install dependencies (Python 3.11+ with `ffmpeg`/`ffprobe` on `PATH`):

```bash
# Create virtual environment
python3.11 -m venv venv
source venv/bin/activate

# Install the package
pip install -e .

Process a video end-to-end:

```bash
./run_production_local.sh /path/to/video.mp4 --parts 16
```

Or use the CLI directly:

```bash
ai-video-editor-local /path/to/video.mp4 \
  --output-dir ./workspace \
  --parts 16
```

### For GPU/CPU (Alternative)

If you have a powerful GPU

```bash
ai-video-editor-local /path/to/video.mp4 \
  --output-dir ./workspace \
  --analysis-model mlx-community/Qwen3-VL-30B-A3B-Thinking-4bit \
  --device cuda \
  --torch-dtype float16
```

Key outputs (under `--output-dir`, defaults beside the source video):

- `chunks/` – raw chunk files from the splitter.
- `analysis/` – one markdown file per chunk plus `people.md`.
- `plan/plan.json` & `plan.md` – structured edit plan (passes schema used by the editor).
- `edited/` – edited chunks generated by ffmpeg.
- `final_video.mp4` – concatenated finished video.
- music library – provide MP3/WAV loops under `music/` (relative to your working directory) or point `--music-dir` at another folder for the planner/editor to use.

Flags of note:

- `--skip-analysis`, `--skip-planning`, `--skip-editing` let you resume partial runs.
- `--font-path` points `drawtext` overlays to a specific font if the macOS default is unavailable.
- `--music-dir` selects the folder of background loops to blend into segments.
- `--use-file-picker` opens a Tk-based selector when you'd rather not pass the path on the CLI.
- `--parts` controls how many chunks to split the video into (default: 4, recommend 16 for long videos)


## Development

- Run the unit tests (requires `pytest`): `pytest`
- Linting/formatting is left to your preference; the project stays black/ruff-compatible.
- `--dry-run` in the CLI writes placeholder files so you can validate the orchestration without real media or API calls.

## Troubleshooting

### Memory Issues on Mac
If you encounter memory errors:
1. Increase `--parts` to create smaller chunks (try 16 or 32)
2. Close other applications to free up RAM
3. Ensure you have at least 64GB RAM for the 30B model

### Model Download
First run will download models:
- Models are cached in `~/.cache/huggingface/`
- Download time: 30m minutes depending on connection
- Subsequent runs use cached models
