# AI Video Editor

This project aims to take a single long-form source video and automatically craft an edited cut using ffmpeg driven by large language models.

## Two Ways to Run

### 1. **Local Models** (Recommended - No API costs!)
Run everything on your own hardware using HuggingFace models:
- ✅ **Free** - No API costs
- ✅ **Private** - Videos never leave your machine
- ✅ **Offline** - Works without internet (after model download)
- See **[LOCAL_MODELS_GUIDE.md](LOCAL_MODELS_GUIDE.md)** for setup

### 2. **DashScope API** (Original)
Use Alibaba Cloud's API service:
- Fast cloud-based inference
- Requires API key and credits
- See **[SETUP_COMPLETE.md](SETUP_COMPLETE.md)** for setup

## Ingredients

### Local Models (Recommended)
- Video analysis: Qwen3-VL-30B-A3B-Instruct
- Planning: `Qwen/Qwen3-39B-Instruct` (or any HuggingFace text model)
- Framework: [HuggingFace Transformers](https://huggingface.co/docs/transformers)

### DashScope API (Original)
- Video analysis model: `Qwen/Qwen3-VL-30B-A3B-Instruct`
- Planning / editing agent model: `Qwen3-30B-A3B`
- Agent framework: [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent)
- Recipe inspiration: [Qwen3-VL video understanding cookbook](https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/video_understanding.ipynb)

## Features

The CLI currently automates the entire post-production loop:

1. **Chunking** – Splits a long source video into evenly sized segments via `ffmpeg`.
2. **Chunk analysis** – Invokes `Qwen/Qwen3-VL-30B-A3B-Instruct` to describe every segment, highlight dull moments, and capture people metadata into markdown.
3. **Director plan** – Summarises all chunk reports with `Qwen3-30B-A3B` to produce a structured editing script (JSON + Markdown).
4. **ffmpeg execution** – Applies trims, optional crops, speed ramps, overlays, subtitles, and background music cues (mixed from the local royalty-free library) to produce polished chunk edits.
5. **Final assembly** – Concatenates the edited chunks into one final deliverable.

Use `--dry-run` to exercise the flow without hitting remote APIs or ffmpeg (helpful in CI or when the models are unavailable).

## Quick start

Install dependencies (Python 3.10+ with `ffmpeg`/`ffprobe` on `PATH`):

```bash
pip install -e .
```

Set your DashScope key (`DASHSCOPE_API_KEY`) or pass `--api-key`.

Process a video end-to-end (the tool auto-discovers royalty-free loops in `./music` unless you override `--music-dir`):

```bash
ai-video-editor /path/to/video.mp4 \
  --output-dir ./workspace \
  --analysis-model Qwen/Qwen3-VL-30B-A3B-Instruct \
  --planner-model Qwen3-30B-A3B
```

Key outputs (under `--output-dir`, defaults beside the source video):

- `chunks/` – raw chunk files from the splitter.
- `analysis/` – one markdown file per chunk plus `people.md`.
- `plan/plan.json` & `plan.md` – structured edit plan (passes schema used by the editor).
- `new/` – edited chunks generated by ffmpeg.
- `final_video.mp4` – concatenated finished video.
- music library – provide MP3/WAV loops under `music/` (relative to your working directory) or point `--music-dir` at another folder for the planner/editor to use.

Flags of note:

- `--skip-analysis`, `--skip-planning`, `--skip-editing` let you resume partial runs.
- `--font-path` points `drawtext` overlays to a specific font if the macOS default is unavailable.
- `--music-dir` selects the folder of background loops to blend into segments.
- `--use-file-picker` opens a Tk-based selector when you’d rather not pass the path on the CLI.

## Development

- Run the unit tests (requires `pytest`): `pytest`
- Linting/formatting is left to your preference; the project stays black/ruff-compatible.
- `--dry-run` in the CLI writes placeholder files so you can validate the orchestration without real media or API calls.

## Roadmap ideas

- Enrich prompt engineering for more precise overlays and camera direction.
- Add voice-over synthesis hooks aligned with the generated scripts.
- Bundle optional assets (fonts, LUTs) and ship a Docker/devcontainer for reproducible environments.
