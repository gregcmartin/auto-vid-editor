# AI Video Editor

This project aims to take a single long-form source video and automatically craft an edited cut using ffmpeg driven by large language models.

## Running Locally with MLX (Apple Silicon)

**✅ Recommended for Mac users** - Uses optimized MLX models via NexaSDK:
- ✅ **Free** - No API costs
- ✅ **Private** - Videos never leave your machine
- ✅ **Optimized** - 4-bit quantized models for Apple Silicon
- ✅ **Memory Efficient** - ~8GB model size vs 60GB
- See **[LOCAL_MODELS_GUIDE.md](LOCAL_MODELS_GUIDE.md)** for setup

## Ingredients

### MLX Models (Apple Silicon - Recommended)
- **Video analysis: `NexaAI/qwen3vl-30B-A3B-mlx`** (4-bit quantized, ~8GB)
- **Planning: `Qwen/Qwen3-30B-A3B-MLX-8bit`** (8-bit quantized for Apple Silicon)
- Framework: [NexaSDK](https://github.com/NexaAI/nexa-sdk) + [MLX](https://github.com/ml-explore/mlx)
- **Requirements**: Mac with Apple Silicon (M1/M2/M3), 64GB RAM recommended

### PyTorch Models (GPU/CPU - Alternative)
- **Video analysis: `Qwen/Qwen3-VL-30B-A3B-Instruct`** (requires 128GB+ RAM or cloud GPU)
- **Planning: `Qwen/Qwen3-30B-A3B-MLX-8bit`**
- Framework: [HuggingFace Transformers](https://huggingface.co/docs/transformers)
- **Note**: These are Qwen3 models ONLY. Do not use Qwen2 models.

## Features

The CLI currently automates the entire post-production loop:

1. **Chunking** – Splits a long source video into evenly sized segments via `ffmpeg`.
2. **Chunk analysis** – Invokes `NexaAI/qwen3vl-30B-A3B-mlx` (or PyTorch alternative) to describe every segment, highlight dull moments, and capture people metadata into markdown.
3. **Director plan** – Summarises all chunk reports with `Qwen3-30B-A3B` to produce a structured editing script (JSON + Markdown).
4. **ffmpeg execution** – Applies trims, optional crops, speed ramps, overlays, subtitles, and background music cues (mixed from the local royalty-free library) to produce polished chunk edits.
5. **Final assembly** – Concatenates the edited chunks into one final deliverable.

## Quick start

### For Apple Silicon (Recommended)

Install dependencies (Python 3.11+ with `ffmpeg`/`ffprobe` on `PATH`):

```bash
# Create virtual environment
python3.11 -m venv venv
source venv/bin/activate

# Install the package
pip install -e .

# Install NexaSDK for MLX support
pip install nexaai
```

Process a video end-to-end:

```bash
./run_production_local.sh /path/to/video.mp4 --parts 16
```

Or use the CLI directly:

```bash
ai-video-editor-local /path/to/video.mp4 \
  --output-dir ./workspace \
  --parts 16
```

### For GPU/CPU (Alternative)

If you have a powerful GPU or cloud instance:

```bash
ai-video-editor-local /path/to/video.mp4 \
  --output-dir ./workspace \
  --analysis-model Qwen/Qwen3-VL-30B-A3B-Instruct \
  --device cuda \
  --torch-dtype float16
```

Key outputs (under `--output-dir`, defaults beside the source video):

- `chunks/` – raw chunk files from the splitter.
- `analysis/` – one markdown file per chunk plus `people.md`.
- `plan/plan.json` & `plan.md` – structured edit plan (passes schema used by the editor).
- `edited/` – edited chunks generated by ffmpeg.
- `final_video.mp4` – concatenated finished video.
- music library – provide MP3/WAV loops under `music/` (relative to your working directory) or point `--music-dir` at another folder for the planner/editor to use.

Flags of note:

- `--skip-analysis`, `--skip-planning`, `--skip-editing` let you resume partial runs.
- `--font-path` points `drawtext` overlays to a specific font if the macOS default is unavailable.
- `--music-dir` selects the folder of background loops to blend into segments.
- `--use-file-picker` opens a Tk-based selector when you'd rather not pass the path on the CLI.
- `--parts` controls how many chunks to split the video into (default: 4, recommend 16 for long videos)

## Model Information

### Why MLX/NexaSDK?

The MLX framework is specifically optimized for Apple Silicon, providing:
- **4-bit quantization**: Reduces model size from 60GB to ~8GB
- **Unified memory**: Efficiently uses Mac's unified memory architecture
- **Native optimization**: Built for M1/M2/M3 chips
- **No huge buffers**: Streams weights without massive allocations

### Model Comparison

| Model | Size | Memory | Speed | Platform |
|-------|------|--------|-------|----------|
| NexaAI/qwen3vl-30B-A3B-mlx | ~8GB | 64GB RAM | Fast | Mac M1/M2/M3 |
| Qwen3-VL-30B-A3B-Instruct | ~60GB | 128GB+ RAM | Slow | GPU/Cloud |

## Development

- Run the unit tests (requires `pytest`): `pytest`
- Linting/formatting is left to your preference; the project stays black/ruff-compatible.
- `--dry-run` in the CLI writes placeholder files so you can validate the orchestration without real media or API calls.

## Troubleshooting

### Memory Issues on Mac
If you encounter memory errors:
1. Increase `--parts` to create smaller chunks (try 16 or 32)
2. Close other applications to free up RAM
3. Ensure you have at least 64GB RAM for the 30B model

### Model Download
First run will download models (~8GB for MLX version):
- Models are cached in `~/.cache/huggingface/`
- Download time: 5-15 minutes depending on connection
- Subsequent runs use cached models

## Roadmap ideas

- Enrich prompt engineering for more precise overlays and camera direction.
- Add voice-over synthesis hooks aligned with the generated scripts.
- Bundle optional assets (fonts, LUTs) and ship a Docker/devcontainer for reproducible environments.
- Support for smaller quantized models (4B/7B variants when available)
